#!/usr/bin/env bash
set -euo pipefail
trap "exit" INT
THIS_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Runs smoke tests - basic use-case checks with default data
# https://en.wikipedia.org/wiki/Smoke_testing_(software)
#
# Dependencies:
#   sudo apt-get install -y bash curl parallel
#
#   curl -fsSL "https://github.com/jqlang/jq/releases/download/jq-1.7.1/jq-linux-amd64" -o "${HOME}/bin/jq" && chmod +x "${HOME}/bin/jq"
#
#   curl -sSL "https://github.com/shenwei356/seqkit/releases/download/v2.5.0/seqkit_linux_amd64.tar.gz" | tar -C "${HOME}/bin" -xz "seqkit"
#
# Usage (NOTE: you must build and re-build Nextclade executable yourself, this script does not do that):
#
# 1. Download datasets from the default dataset server and run tests with a given nextclade executable:
#
#     ./tests/run-smoke-tests 'target/release/nextclade'
#
# The downloaded datasets will be in "${DATASETS_DIR}" and Nextclade output files will be in "${RESULTS_DIR}" (see below)
#
#
# 2. Run tests with a given nextclade executable and a directory containing datasets. Dataset directories are
#    identified as directories containing a 'pathogen.json' file.
#
#     ./tests/run-smoke-tests 'target/release/nextclade' '.../nextclade_data/data_output'
#
# Options:
#   -j, --jobs <N>        Total jobs (default: JOBS env var, or nproc)
#   --datasets-dir <dir>  Directory for datasets (default: DATASETS_DIR env var)
#   --results-dir <dir>   Directory for results (default: RESULTS_DIR env var)

# Parse arguments
NEXTCLADE_BIN=""
INPUT_DATASETS_DIR=""
JOBS_ARG=""
DATASETS_DIR_ARG=""
RESULTS_DIR_ARG=""

while [[ $# -gt 0 ]]; do
  case "${1}" in
    -j|--jobs)
      JOBS_ARG="${2}"
      shift 2
      ;;
    --datasets-dir)
      DATASETS_DIR_ARG="${2}"
      shift 2
      ;;
    --results-dir)
      RESULTS_DIR_ARG="${2}"
      shift 2
      ;;
    -*)
      printf "Unknown option: %s\n" "${1}" >&2
      exit 1
      ;;
    *)
      if [[ -z "${NEXTCLADE_BIN}" ]]; then
        NEXTCLADE_BIN="${1}"
      elif [[ -z "${INPUT_DATASETS_DIR}" ]]; then
        INPUT_DATASETS_DIR="${1}"
      else
        printf "Unexpected argument: %s\n" "${1}" >&2
        exit 1
      fi
      shift
      ;;
  esac
done

if [[ -z "${NEXTCLADE_BIN}" ]]; then
  printf "Usage: %s [options] <path_to_nextclade> [path_to_dataset_collection_dir]\n" "${0}" >&2
  printf "Options:\n" >&2
  printf "  -j, --jobs <N>        Total jobs (default: JOBS env var, or nproc)\n" >&2
  printf "  --datasets-dir <dir>  Directory for datasets (default: DATASETS_DIR env var)\n" >&2
  printf "  --results-dir <dir>   Directory for results (default: RESULTS_DIR env var)\n" >&2
  exit 1
fi

export NEXTCLADE_BIN
export INPUT_DATASETS_DIR

# Determine total jobs: CLI arg > env var > nproc
TOTAL_JOBS="${JOBS_ARG:-${JOBS:-$(nproc)}}"

# Split threads between parallel and nextclade to avoid over-subscription
case "${TOTAL_JOBS}" in
  1)
    PARALLEL_JOBS=1
    NEXTCLADE_JOBS=1
    ;;
  2|3)
    PARALLEL_JOBS=2
    NEXTCLADE_JOBS=2
    ;;
  *)
    read -r PARALLEL_JOBS NEXTCLADE_JOBS < <(python3 -c "
import math
C = ${TOTAL_JOBS}
P = int(math.sqrt(C))
P = P if C % P == 0 else P - 1 if P > 1 and C % (P - 1) == 0 else P
print(C // P, P)
")
    ;;
esac

export PARALLEL_JOBS
export NEXTCLADE_JOBS

printf "Using %d parallel jobs with %d nextclade threads each (total: %d)\n" "${PARALLEL_JOBS}" "${NEXTCLADE_JOBS}" "$((PARALLEL_JOBS * NEXTCLADE_JOBS))"

# Determine directories: CLI arg > env var > default
export DATASETS_DIR="${DATASETS_DIR_ARG:-${DATASETS_DIR:-${THIS_DIR}/../tmp/smoke-tests/dataset}}"
export RESULTS_DIR="${RESULTS_DIR_ARG:-${RESULTS_DIR:-${THIS_DIR}/../tmp/smoke-tests/result}}"

function run_with_name() {
  local name="${1}"
  local sequences="${2}"
  local out_dir="${RESULTS_DIR}/${name}/with_name"

  "${NEXTCLADE_BIN}" run --quiet --jobs="${NEXTCLADE_JOBS}" --retry-reverse-complement --in-order --include-reference \
    --dataset-name="${name}" \
    --output-translations="${out_dir}/translations/{cds}.translation.fasta" \
    --output-all="${out_dir}" \
    "${sequences}"
}
export -f run_with_name

function run_with_dataset_dir() {
  local name="${1}"
  local dataset_dir="${2}"
  local sequences="${3}"
  local out_dir="${RESULTS_DIR}/${name}/with_dataset"

  "${NEXTCLADE_BIN}" run --quiet --jobs="${NEXTCLADE_JOBS}" --retry-reverse-complement --in-order --include-reference \
    --input-dataset="${dataset_dir}" \
    --output-translations="${out_dir}/translations/{cds}.translation.fasta" \
    --output-all="${out_dir}" \
    "${sequences}"
}
export -f run_with_dataset_dir

function run_with_dataset_zip() {
  local name="${1}"
  local zip_path="${2}"
  local sequences="${3}"
  local out_dir="${RESULTS_DIR}/${name}/with_dataset_zip"

  "${NEXTCLADE_BIN}" run --quiet --jobs="${NEXTCLADE_JOBS}" --retry-reverse-complement --in-order --include-reference \
    --input-dataset="${zip_path}" \
    --output-translations="${out_dir}/translations/{cds}.translation.fasta" \
    --output-all="${out_dir}" \
    "${sequences}"
}
export -f run_with_dataset_zip

function run_with_ref_only() {
  local name="${1}"
  local dataset_dir="${2}"
  local sequences="${3}"
  local out_dir="${RESULTS_DIR}/${name}/with_ref_only"

  if [ ! -f "${dataset_dir}/reference.fasta" ]; then return; fi

  "${NEXTCLADE_BIN}" run --quiet --jobs="${NEXTCLADE_JOBS}" --retry-reverse-complement --in-order --include-reference \
    --input-ref="${dataset_dir}/reference.fasta" \
    --output-translations="${out_dir}/translations/{cds}.translation.fasta" \
    --output-all="${out_dir}" \
    "${sequences}"
}
export -f run_with_ref_only

function run_with_ref_and_annotation() {
  local name="${1}"
  local dataset_dir="${2}"
  local sequences="${3}"
  local out_dir="${RESULTS_DIR}/${name}/with_ref_and_annotation"

  if [ ! -f "${dataset_dir}/reference.fasta" ]; then return; fi
  if [ ! -f "${dataset_dir}/genome_annotation.gff3" ]; then return; fi

  "${NEXTCLADE_BIN}" run --quiet --jobs="${NEXTCLADE_JOBS}" --retry-reverse-complement --in-order --include-reference \
    --input-ref="${dataset_dir}/reference.fasta" \
    --input-annotation="${dataset_dir}/genome_annotation.gff3" \
    --output-translations="${out_dir}/translations/{cds}.translation.fasta" \
    --output-all="${out_dir}" \
    "${sequences}"
}
export -f run_with_ref_and_annotation

function run_with_ref_and_tree() {
  local name="${1}"
  local dataset_dir="${2}"
  local sequences="${3}"
  local out_dir="${RESULTS_DIR}/${name}/with_ref_and_tree"

  if [ ! -f "${dataset_dir}/reference.fasta" ]; then return; fi
  if [ ! -f "${dataset_dir}/tree.json" ]; then return; fi

  "${NEXTCLADE_BIN}" run --quiet --jobs="${NEXTCLADE_JOBS}" --retry-reverse-complement --in-order --include-reference \
    --input-ref="${dataset_dir}/reference.fasta" \
    --input-tree="${dataset_dir}/tree.json" \
    --output-translations="${out_dir}/translations/{cds}.translation.fasta" \
    --output-all="${out_dir}" \
    "${sequences}"
}
export -f run_with_ref_and_tree

function run_with_ref_and_annotation_and_tree() {
  local name="${1}"
  local dataset_dir="${2}"
  local sequences="${3}"
  local out_dir="${RESULTS_DIR}/${name}/with_ref_and_annotation_and_tree"

  if [ ! -f "${dataset_dir}/genome_annotation.gff3" ]; then return; fi
  if [ ! -f "${dataset_dir}/tree.json" ]; then return; fi

  "${NEXTCLADE_BIN}" run --quiet --jobs="${NEXTCLADE_JOBS}" --retry-reverse-complement --in-order --include-reference \
    --input-ref="${dataset_dir}/reference.fasta" \
    --input-annotation="${dataset_dir}/genome_annotation.gff3" \
    --input-tree="${dataset_dir}/tree.json" \
    --output-translations="${out_dir}/translations/{cds}.translation.fasta" \
    --output-all="${out_dir}" \
    "${sequences}"
}
export -f run_with_ref_and_annotation_and_tree

function get_sequence_file() {
  local dataset_dir="${1}"
  local target_dir="${2}"

  local sequences
  sequences="$(jq -re ".files.examples | select(length > 0)" "${dataset_dir}/pathogen.json" 2>/dev/null || printf "")"
  local msg_no_sequences=""
  if [ -n "${sequences}" ] && [ -f "${dataset_dir}/${sequences}" ]; then
    if [ "${target_dir}" != "${dataset_dir}" ]; then
      cp "${dataset_dir}/${sequences}" "${target_dir}/"
    fi
    sequences="${target_dir}/${sequences}"
  else
    if [ "${target_dir}" != "${dataset_dir}" ]; then
      cp "${dataset_dir}/reference.fasta" "${target_dir}/"
    fi
    sequences="${target_dir}/reference.fasta"
    msg_no_sequences="1"
  fi

  printf "%s|%s" "${sequences}" "${msg_no_sequences}"
}
export -f get_sequence_file

function add_reverse_complement() {
  local sequences="${1}"

  if command -v seqkit &>/dev/null; then
    local sequences_orig="${sequences%.fasta}.orig.fasta"
    local sequences_rev_comp="${sequences%.fasta}.rev.fasta"
    cp "${sequences}" "${sequences_orig}"
    seqkit seq --quiet -t DNA -p -r "${sequences_orig}" | seqkit replace -p '(.*)' -r 'SMOKE_TEST_REVERSE_COMPLEMENTED_SEQUENCE|$1' >"${sequences_rev_comp}"
    (
      cat "${sequences_orig}"
      printf "\n"
      cat "${sequences_rev_comp}"
    ) >"${sequences}"
  fi
}
export -f add_reverse_complement

function download_and_run_dataset() {
  local name="${1}"
  local dataset_dir="${DATASETS_DIR}/dir/${name}"
  local zip_path="${DATASETS_DIR}/zip/${name}/dataset.zip"

  "${NEXTCLADE_BIN}" dataset get --name="${name}" --output-dir="${dataset_dir}"
  "${NEXTCLADE_BIN}" dataset get --name="${name}" --output-zip="${zip_path}"

  local result
  result=$(get_sequence_file "${dataset_dir}" "${dataset_dir}")
  local sequences="${result%|*}"
  local has_warning="${result#*|}"

  add_reverse_complement "${sequences}"

  if [ "${has_warning}" = "1" ]; then
    printf "\nRunning '%s' for '%s'\n\e[93mWarning: dataset '%s' contains no example sequences. Will use reference sequence as query input.\e[0m\n" "${NEXTCLADE_BIN}" "${name}" "${name}"
  else
    printf "\nRunning '%s' for '%s'\n" "${NEXTCLADE_BIN}" "${name}"
  fi

  # Run with dataset name (only for downloaded datasets)
  run_with_name "${name}" "${sequences}"

  # Run directory-based tests
  run_with_dataset_dir "${name}" "${dataset_dir}" "${sequences}"
  run_with_ref_only "${name}" "${dataset_dir}" "${sequences}"
  run_with_ref_and_annotation "${name}" "${dataset_dir}" "${sequences}"
  run_with_ref_and_tree "${name}" "${dataset_dir}" "${sequences}"
  run_with_ref_and_annotation_and_tree "${name}" "${dataset_dir}" "${sequences}"

  # If dataset.zip exists, also run zip-based test
  if [ -f "${dataset_dir}/dataset.zip" ]; then
    run_with_dataset_zip "${name}" "${dataset_dir}/dataset.zip" "${sequences}"
  fi
}
export -f download_and_run_dataset

function run_local_dataset() {
  local dataset_dir="${1}"

  # Extract dataset name as path relative to INPUT_DATASETS_DIR
  local abs_input_dir
  abs_input_dir="$(cd "${INPUT_DATASETS_DIR}" && pwd)"
  local abs_dataset_dir
  abs_dataset_dir="$(cd "${dataset_dir}" && pwd)"
  local name
  name="$(printf "%s" "${abs_dataset_dir}" | sed "s|^${abs_input_dir}/||")"

  # Create organized directory for this dataset's sequences
  local work_dataset_dir="${DATASETS_DIR}/local/${name}"
  mkdir -p "${work_dataset_dir}"

  local result
  result=$(get_sequence_file "${dataset_dir}" "${work_dataset_dir}")
  local sequences="${result%|*}"
  local has_warning="${result#*|}"

  add_reverse_complement "${sequences}"

  if [ "${has_warning}" = "1" ]; then
    printf "\nRunning '%s' for '%s'\n\e[93mWarning: dataset '%s' contains no example sequences. Will use reference sequence as query input.\e[0m\n" "${NEXTCLADE_BIN}" "${name}" "${name}"
  else
    printf "\nRunning '%s' for '%s'\n" "${NEXTCLADE_BIN}" "${name}"
  fi

  # Run directory-based tests
  run_with_dataset_dir "${name}" "${dataset_dir}" "${sequences}"
  run_with_ref_only "${name}" "${dataset_dir}" "${sequences}"
  run_with_ref_and_annotation "${name}" "${dataset_dir}" "${sequences}"
  run_with_ref_and_tree "${name}" "${dataset_dir}" "${sequences}"
  run_with_ref_and_annotation_and_tree "${name}" "${dataset_dir}" "${sequences}"

  # If dataset.zip exists, also run zip-based test
  if [ -f "${dataset_dir}/dataset.zip" ]; then
    run_with_dataset_zip "${name}" "${dataset_dir}/dataset.zip" "${sequences}"
  fi
}
export -f run_local_dataset

if [ -n "${INPUT_DATASETS_DIR}" ]; then
  printf "Running smoke tests with datasets from: %s\n" "${INPUT_DATASETS_DIR}"

  if [ ! -d "${INPUT_DATASETS_DIR}" ]; then
    printf "Error: Dataset directory '%s' does not exist\n" "${INPUT_DATASETS_DIR}"
    exit 1
  fi

  # Find all directories containing pathogen.json files
  mapfile -t -d '' dataset_dirs < <(find "${INPUT_DATASETS_DIR}" -name "pathogen.json" -type f -printf '%h\0' | sort -zu)

  if ((${#dataset_dirs[@]} == 0)); then
    printf "Error: No datasets found (directories containing pathogen.json files) in '%s'\n" "${INPUT_DATASETS_DIR}"
    exit 1
  fi

  printf "Found %d datasets\n" "${#dataset_dirs[@]}"

  parallel --jobs="${PARALLEL_JOBS}" run_local_dataset ::: "${dataset_dirs[@]}"
else
  printf "Downloading datasets and running smoke tests\n"
  all_datasets="$("${NEXTCLADE_BIN}" dataset list --include-deprecated --only-names)"
  printf "Will download %d datasets\n" "$(printf "%s" "${all_datasets}" | wc -l)"
  parallel --jobs="${PARALLEL_JOBS}" download_and_run_dataset ::: "${all_datasets}"
fi
